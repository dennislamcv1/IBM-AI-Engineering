{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Import Libraries and Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item32'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "For your convenience, I have placed the data on a server which you can retrieve easily using the **wget** command. So let's run the following line of code to get the data. Given the large size of the image dataset, it might take some time depending on your internet speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "## get the data\n",
    "#!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And now if you check the left directory pane, you should see the zipped file *concrete_data_week3.zip* appear. So, let's go ahead and unzip the file to access the images. Given the large number of images in the dataset, this might take a couple of minutes, so please be patient, and wait until the code finishes running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!unzip concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50*** error. So please **DO NOT DO IT**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item33'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Define Global Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item34'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Construct ImageDataGenerator Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item35'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Build, Compile and Fit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x1e7e3f39908>,\n",
       " <keras.layers.core.Dense at 0x1e7ecf1d488>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can access the ResNet50 layers by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x1e7cda738c8>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x1e7cccbfac8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7cda70a08>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7ce03edc8>,\n",
       " <keras.layers.core.Activation at 0x1e7ce03ef48>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x1e7cdc4c3c8>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x1e7ce3541c8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7ce3f6388>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7ce3fe688>,\n",
       " <keras.layers.core.Activation at 0x1e7ce3fe488>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7ce40b208>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7ce4bb1c8>,\n",
       " <keras.layers.core.Activation at 0x1e7ce50c388>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7ce506208>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7ce616188>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7ce5be548>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7ce69b6c8>,\n",
       " <keras.layers.merge.Add at 0x1e7ce714cc8>,\n",
       " <keras.layers.core.Activation at 0x1e7ce71d688>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7ce763708>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7ce8218c8>,\n",
       " <keras.layers.core.Activation at 0x1e7ce821e88>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7ce82ff08>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7ce8e51c8>,\n",
       " <keras.layers.core.Activation at 0x1e7ce930ec8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7ce935188>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7cec47f48>,\n",
       " <keras.layers.merge.Add at 0x1e7cec47ec8>,\n",
       " <keras.layers.core.Activation at 0x1e7cec4e388>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7cec93788>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7ced55688>,\n",
       " <keras.layers.core.Activation at 0x1e7ced55708>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7ced5dbc8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7cee14848>,\n",
       " <keras.layers.core.Activation at 0x1e7cee6ee08>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7cee67688>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7cef17cc8>,\n",
       " <keras.layers.merge.Add at 0x1e7cef742c8>,\n",
       " <keras.layers.core.Activation at 0x1e7cef6e4c8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7cefeef48>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7cf075748>,\n",
       " <keras.layers.core.Activation at 0x1e7cf075e08>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7cf085148>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d0154c88>,\n",
       " <keras.layers.core.Activation at 0x1e7d0154c48>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d015a888>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d066ddc8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d0622808>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d06f6a08>,\n",
       " <keras.layers.merge.Add at 0x1e7d0782288>,\n",
       " <keras.layers.core.Activation at 0x1e7d077dd08>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d07e0048>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d0885688>,\n",
       " <keras.layers.core.Activation at 0x1e7d0885708>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d088cc08>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d0942888>,\n",
       " <keras.layers.core.Activation at 0x1e7d099ce48>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d09946c8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d0aa2308>,\n",
       " <keras.layers.merge.Add at 0x1e7d0aa2088>,\n",
       " <keras.layers.core.Activation at 0x1e7d0a9d5c8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d0b1bfc8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d0ba6788>,\n",
       " <keras.layers.core.Activation at 0x1e7d0ba6f08>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d0bb5848>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d0cb2cc8>,\n",
       " <keras.layers.core.Activation at 0x1e7d0cb2c88>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d0cb9f88>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d0d70848>,\n",
       " <keras.layers.merge.Add at 0x1e7d0dc8e08>,\n",
       " <keras.layers.core.Activation at 0x1e7d0dbf7c8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d0e29048>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d0ecc808>,\n",
       " <keras.layers.core.Activation at 0x1e7d0ecca08>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d0edda48>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d0f564c8>,\n",
       " <keras.layers.core.Activation at 0x1e7d0fe1e08>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d0fdadc8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d18f1cc8>,\n",
       " <keras.layers.merge.Add at 0x1e7d18f1c88>,\n",
       " <keras.layers.core.Activation at 0x1e7d18f92c8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d1941408>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d19f9ac8>,\n",
       " <keras.layers.core.Activation at 0x1e7d19f9e48>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d1a08388>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d1b08f08>,\n",
       " <keras.layers.core.Activation at 0x1e7d1b08dc8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d1b10848>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d2c26088>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d2bd49c8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d2ca6308>,\n",
       " <keras.layers.merge.Add at 0x1e7d2d39148>,\n",
       " <keras.layers.core.Activation at 0x1e7d2d32dc8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d2d91588>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d2e3a808>,\n",
       " <keras.layers.core.Activation at 0x1e7d2e3a9c8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d2e4ea88>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d2f40f88>,\n",
       " <keras.layers.core.Activation at 0x1e7d2f50408>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d2f472c8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d3006548>,\n",
       " <keras.layers.merge.Add at 0x1e7d3050cc8>,\n",
       " <keras.layers.core.Activation at 0x1e7d3057608>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d309f448>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d3157f08>,\n",
       " <keras.layers.core.Activation at 0x1e7d3157dc8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d3166388>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d321b5c8>,\n",
       " <keras.layers.core.Activation at 0x1e7d3265d48>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d326d3c8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d336dd08>,\n",
       " <keras.layers.merge.Add at 0x1e7d336de48>,\n",
       " <keras.layers.core.Activation at 0x1e7d3376d88>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d33d9248>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d347ff08>,\n",
       " <keras.layers.core.Activation at 0x1e7d347ffc8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d348e548>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d3542248>,\n",
       " <keras.layers.core.Activation at 0x1e7d3590e88>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d35944c8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d576a588>,\n",
       " <keras.layers.merge.Add at 0x1e7d57b5e48>,\n",
       " <keras.layers.core.Activation at 0x1e7d57bc508>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d5801848>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d58c5908>,\n",
       " <keras.layers.core.Activation at 0x1e7d58c5648>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d58cdc48>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d59caf48>,\n",
       " <keras.layers.core.Activation at 0x1e7d59da488>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d59d2f08>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d5a92208>,\n",
       " <keras.layers.merge.Add at 0x1e7d5ae3388>,\n",
       " <keras.layers.core.Activation at 0x1e7d5ade608>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d5b5bd08>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d5be3dc8>,\n",
       " <keras.layers.core.Activation at 0x1e7d5be3f08>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d5bf2f08>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d5ca8788>,\n",
       " <keras.layers.core.Activation at 0x1e7d5cf1dc8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d5cf9e08>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d5daf8c8>,\n",
       " <keras.layers.merge.Add at 0x1e7d5e07e88>,\n",
       " <keras.layers.core.Activation at 0x1e7d5e00708>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d5e64088>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d5f0bc08>,\n",
       " <keras.layers.core.Activation at 0x1e7d5f0bcc8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d5f1dac8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7d5fc4a88>,\n",
       " <keras.layers.core.Activation at 0x1e7d6021108>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7d6018288>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7db3165c8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7db2c6588>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7db39f548>,\n",
       " <keras.layers.merge.Add at 0x1e7db419ac8>,\n",
       " <keras.layers.core.Activation at 0x1e7db41ec48>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7db468748>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7db523808>,\n",
       " <keras.layers.core.Activation at 0x1e7db523fc8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7db531e48>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7db5ee588>,\n",
       " <keras.layers.core.Activation at 0x1e7db632e08>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7db639588>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7db6ed908>,\n",
       " <keras.layers.merge.Add at 0x1e7db747648>,\n",
       " <keras.layers.core.Activation at 0x1e7db740208>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7db7a7088>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7db849c48>,\n",
       " <keras.layers.core.Activation at 0x1e7db849d08>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7db85ba08>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7db907ac8>,\n",
       " <keras.layers.core.Activation at 0x1e7db960d48>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e7db959288>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x1e7e3e375c8>,\n",
       " <keras.layers.merge.Add at 0x1e7e3e80dc8>,\n",
       " <keras.layers.core.Activation at 0x1e7e3e86688>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x1e7db7a7c08>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\"resnet.h5\",monitor='val_loss',verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next we compile our model using the **adam** optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)/60  #Added due to long training times\n",
    "steps_per_epoch_validation = len(validation_generator)/20 #Added due to long training times\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "5/5 [==============================] - 381s 76s/step - loss: 0.1566 - accuracy: 0.9740 - val_loss: 0.3323 - val_accuracy: 0.7980\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.36317 to 0.33226, saving model to resnet.h5\n"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:384: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "resnet = load_model(\"resnet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
